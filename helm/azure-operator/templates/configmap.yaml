apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "resource.default.name"  . }}
  namespace: {{ include "resource.default.namespace"  . }}
  labels:
    {{- include "labels.common" . | nindent 4 }}
data:
  config.yaml: |
    server:
      enable:
        debug:
          server: true
      listen:
        address: 'http://0.0.0.0:8000'
    service:
      azure:
        environmentName: '{{ .Values.azure.environmentName }}'
        hostCluster:
          cidr: '{{ .Values.azure.managementClusterCluster.cidr }}'
          resourceGroup: '{{ .Values.azure.managementClusterCluster.resourceGroup }}'
          virtualNetwork: '{{ .Values.azure.managementClusterCluster.virtualNetwork }}'
          virtualNetworkGateway: '{{ .Values.azure.managementClusterCluster.virtualNetworkGateway }}'
        location: '{{ .Values.azure.location }}'
        msi:
          enabled: {{ .Values.azure.msi.enabled }}
      cluster:
        baseDomain: '{{ .Values.cluster.baseDomain }}'
        calico:
          cidr: '{{ .Values.cluster.cni.mask }}'
          mtu: 1430
          subnet: '{{ .Values.cluster.cni.subnet }}'
        docker:
          daemon:
            cidr: '{{ .Values.cluster.docker.daemon.cidr }}'
            extraArgs: '--log-opt max-size=25m --log-opt max-file=2 --log-opt labels=io.kubernetes.container.hash,io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace,io.kubernetes.pod.uid'
        etcd:
          altNames: ''
          port: 2379
          prefix: 'giantswarm.io'
        kubernetes:
          api:
            altNames: 'kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local'
            clusterIPRange: '172.31.0.0/16'
            securePort: 443
          domain: 'cluster.local'
          ingressController:
            baseDomain:  '{{ .Values.cluster.kubernetes.ingressController.baseDomain }}'
            insecurePort: 30010
            securePort: 30011
          kubelet:
            altNames: 'kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local'
            labels: ''
            port: 10250
          ssh:
            userList: |
              {{ .Values.cluster.kubernetes.ssh.userList | nindent 14 }}
      {{- if .Values.debug }}
      {{- if .Values.debug.insecureStorageAccount }}
      debug:
        insecureStorageAccount: true
      {{- end }}
      {{- end }}
      installation:
        name: '{{ .Values.installation }}'
        guest:
          IPAM:
            Network:
              CIDR: '{{ .Values.workloadCluster.ipam.network.cidr }}'
              subnetMaskBits: '{{ .Values.workloadCluster.ipam.network.subnetMaskBits }}'
        {{- if hasKey .Values.workloadCluster "oidc" }}
        tenant:
          kubernetes:
            api:
              auth:
                provider:
                  oidc:
                    clientID: '{{ .Values.workloadCluster.oidc.clientID }}'
                    issuerURL: '{{ .Values.workloadCluster.oidc.issuerURL }}'
                    usernameClaim: '{{ .Values.workloadCluster.oidc.usernameClaim }}'
                    groupsClaim: '{{ .Values.workloadCluster.oidc.groupsClaim }}'
        {{- end }}
      kubernetes:
        incluster: true
      registry:
        domain: 'docker.io'
        mirrors: 'giantswarm.azurecr.io'
      tenant:
        ssh:
          ssoPublicKey: {{ .Values.workloadCluster.ssh.ssoPublicKey | quote }}
      sentry:
        dsn: 'https://632f9667d01c47719beb5b405962de53@o346224.ingest.sentry.io/5544796'
